---
title: "Applicability domain methods for continuous data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{continuous-data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

```{r}
library(applicable)
```

To analyze the applicability domain of your model, `applicable` provides the following methods:

* Principal component analysis
* Hat values statistics

```{r ames_data, message=FALSE}

library(AmesHousing)
ames <- make_ames()

```

There are `r format(nrow(ames), big.mark = ",")` properties in the data.

The Sale Price was recorded along with `r ncol(ames)` predictors, including:

* Location (e.g. neighborhood) and lot information.
* House components (garage, fireplace, pool, porch, etc.).
* General assessments such as overall quality and condition.
* Number of bedrooms, baths, and so on.

More details can be found in [De Cock (2011, Journal of Statistics Education)](http://ww2.amstat.org/publications/jse/v19n3/decock.pdf).

The raw data are at [`http://bit.ly/2whgsQM`](http://bit.ly/2whgsQM) but we will use a processed version found in the [`AmesHousing`](https://github.com/topepo/AmesHousing) package.

To pre-process the training set, we will use the _recipes_ package. We first eliminate predictors with sparse and highly unbalanced distributions (aka "near-zero variance predictors"), then estimate a transformation that will make the predictor distributions more symmetric. After these, the data are centered and scaled and the PCA components are computed. These same transformations will be applied to the new data points using the statistics estimated from the training set.

```{r prep_data, message=FALSE}

library(recipes)
library(rsample)

set.seed(4595)
data_split <- initial_split(ames, strata = "Sale_Price")

training_data <- training(data_split)
test_data  <- testing(data_split)

training_recipe <-
  recipe( ~ ., data = training_data) %>%
  step_dummy(all_nominal()) %>%
  # Remove variables that have (or almost have) the same value for every data point.
  step_nzv(all_predictors()) %>%
  # Transform variables to be distributed as Gaussian-like as possible.
  step_YeoJohnson(all_predictors()) %>%
  # Normalize numeric data to have a mean of zero.
  step_center(all_predictors()) %>%
  # Normalize numeric data to have a standard deviation of one.
  step_scale(all_predictors()) %>%
  prep(strings_as_factors = FALSE)

```


### Principal component analysis
```{r}

ames_ad <- apd_pca(training_recipe, training_data)
ames_ad

```


```{r, fig.width=5, fig.height=5.2,  out.width = '50%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}

autoplot(ames_ad)

```

The plot contains too much information, making it hard to read. The autoplot function allows us to use selectors, in particular, regular expressions. For example, we can plot exactly the first nine principal components and the last four components. Notice how there is less variance with higher principal components.

```{r, fig.width=5, fig.height=5.2,  out.width = '50%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}

# Using selectors in `...`
autoplot(ames_ad, matches("PC0\\d|PC7\\d"))

```

Alternative, we can print only the distance statistic.

```{r, fig.width=5, fig.height=5.2,  out.width = '50%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}

# Using selectors in `...`
autoplot(ames_ad, distance) + scale_x_log10()

```


The `score` function compares the training data to new samples. Suppose that we have the following data:

```{r fake_sample, echo = FALSE}

training_df <-
  juice(training_recipe)

training_df_ad <- apd_pca(x = training_df)

set.seed(1)
s1 <- sample(x = c(0.1, 0.2, 0.254, 0.123), size = ncol(training_df), replace = TRUE)
fake_sample <- matrix(s1, nrow = 1)
colnames(fake_sample) <- colnames(training_df)
fake_sample <- as.data.frame(fake_sample)

scoring <- score(training_df_ad, fake_sample)

training_pca <- data.frame("PC1" = training_df_ad$pcs$rotation[,1],
                           "PC2" = training_df_ad$pcs$rotation[,2])

testing_pca <- data.frame("PC1" = scoring$PC01,
                          "PC2" = scoring$PC02)

all_pca <- rbind(training_pca, testing_pca)
pca_rng <- extendrange(c(all_pca$PC1, all_pca$PC2), f=c(.03,.01))

training_pca_plot <- ggplot(training_pca, aes(x = PC1, y = PC2, col = data)) +
  geom_point(alpha = .4) +
  lims(x = pca_rng, y = pca_rng) +
  theme(legend.position = "none")

all_pca_plot <- ggplot(all_pca, aes(x = PC1, y = PC2, col = data)) +
    geom_point(alpha = .4) +
    lims(x = pca_rng, y = pca_rng) +
  theme(legend.position = "none")


training_pca_plot <- ggplot(training_pca, aes(x = PC1, y = PC2)) +
  geom_point(alpha = .1) +
  lims(x = pca_rng, y = pca_rng) +
  theme(legend.position = "none")

mean_point <- data.frame("PC1" = mean(training_pca$PC1),
                         "PC2" = mean(training_pca$PC2))

training_pca_plot_with_new_sample_expectation <-
  ggplot(training_pca, aes(x = PC1, y = PC2)) +
  geom_point(alpha = .1) +
  lims(x = pca_rng, y = pca_rng) +
  theme(legend.position = "none") +
  geom_point(data = mean_point, col = "red", cex = 5)

training_pca_plot_with_new_sample_reality <-
  ggplot(training_pca, aes(x = PC1, y = PC2)) +
  geom_point(alpha = .1) +
  lims(x = pca_rng, y = pca_rng) +
  theme(legend.position = "none") +
  geom_point(data = testing_pca, col = "red", cex = 5)

#ggplot(scoring, aes(x = PC01, y = PC01_pctl)) +
#  geom_bar(stat="identity")

#ggplot(scoring, aes(PC01)) + geom_bar(aes(weight = PC01_pctl))

par(mfrow=c(1,2))

training_pca_plot
training_pca_plot_with_new_sample_expectation

```

and we want to predict a new sample.

### Hat values scoring methods

```{r}
predictors <- mtcars[, -1]

# Data frame interface
mod <- apd_hat_values(predictors)

# Formula interface
mod2 <- apd_hat_values(mpg ~ ., mtcars)

# Recipes interface
rec <- recipe(mpg ~ ., mtcars)
rec <- step_log(rec, disp)
mod3 <- apd_hat_values(rec, mtcars)
```
