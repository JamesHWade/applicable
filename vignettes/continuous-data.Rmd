---
title: "Applicability domain methods for continuous data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{continuous-data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, message=FALSE}

library(applicable)
library(ggplot2)
library(dplyr)

```

### Introduction

For our examples, we will use the Ames IA housing data.

```{r ames_data, message=FALSE}

library(AmesHousing)
ames <- make_ames()

```

There are `r format(nrow(ames), big.mark = ",")` properties in the data.

The Sale Price was recorded along with `r ncol(ames)` predictors, including:

* Location (e.g. neighborhood) and lot information.
* House components (garage, fireplace, pool, porch, etc.).
* General assessments such as overall quality and condition.
* Number of bedrooms, baths, and so on.

More details can be found in [De Cock (2011, Journal of Statistics Education)](http://ww2.amstat.org/publications/jse/v19n3/decock.pdf).

The raw data are at [`http://bit.ly/2whgsQM`](http://bit.ly/2whgsQM) but we will use a processed version found in the [`AmesHousing`](https://github.com/topepo/AmesHousing) package.

To pre-process the training set, we will use the _recipes_ package. We first eliminate predictors with sparse and highly unbalanced distributions (aka "near-zero variance predictors"), then estimate a transformation that will make the predictor distributions more symmetric. After these, the data are centered and scaled and the PCA components are computed. These same transformations will be applied to the new data points using the statistics estimated from the training set (e.g. PCA loadings, etc).

```{r prep_data, message=FALSE}

library(recipes)
library(rsample)

set.seed(4595)
data_split <- initial_split(ames, strata = "Sale_Price")

training_data <- training(data_split)
test_data  <- testing(data_split)

training_recipe <-
  recipe( ~ ., data = training_data) %>%
  step_dummy(all_nominal()) %>%
  # Remove variables that have (or almost have) the same value for every data point.
  step_nzv(all_predictors()) %>%
  # Transform variables to be distributed as Gaussian-like as possible.
  step_YeoJohnson(all_predictors()) %>%
  # Normalize numeric data to have a mean of zero.
  step_center(all_predictors()) %>%
  # Normalize numeric data to have a standard deviation of one.
  step_scale(all_predictors()) %>%
  prep(strings_as_factors = FALSE)

sample(training_data)

```


### Principal component analysis
```{r}

ames_ad <- apd_pca(training_recipe, training_data)
ames_ad

```

```{r, fig.width=5, fig.height=5.2,  out.width = '50%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}

autoplot(ames_ad)

```

The plot contains too much information, making it hard to read. The autoplot function allows us to use selectors, in particular, regular expressions. For example, we can plot exactly the first nine principal components:

```{r, fig.width=5, fig.height=5.2,  out.width = '50%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}

# Using selectors in `...`
autoplot(ames_ad, matches("PC0\\d"))

```

```{r, fig.width=5, fig.height=5.2,  out.width = '50%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}

# Using selectors in `...`
autoplot(ames_ad, distance) + scale_x_log10()

```

### Hat values scoring methods
```{r}
predictors <- mtcars[, -1]

# Data frame interface
mod <- apd_hat_values(predictors)

# Formula interface
mod2 <- apd_hat_values(mpg ~ ., mtcars)

# Recipes interface
rec <- recipe(mpg ~ ., mtcars)
rec <- step_log(rec, disp)
mod3 <- apd_hat_values(rec, mtcars)
```
