---
title: "Applicability domain methods for continuous data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{continuous-data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, message=FALSE}

library(applicable)
library(ggplot2)
library(dplyr)

```

### Introduction

For our examples, we will use the Ames IA housing data.

```{r ames_data, message=FALSE}

library(AmesHousing)
ames <- make_ames()

```

There are `r format(nrow(ames), big.mark = ",")` properties in the data.

The Sale Price was recorded along with `r ncol(ames)` predictors, including:

* Location (e.g. neighborhood) and lot information.
* House components (garage, fireplace, pool, porch, etc.).
* General assessments such as overall quality and condition.
* Number of bedrooms, baths, and so on.

More details can be found in [De Cock (2011, Journal of Statistics Education)](http://ww2.amstat.org/publications/jse/v19n3/decock.pdf).

The raw data are at [`http://bit.ly/2whgsQM`](http://bit.ly/2whgsQM) but we will use a processed version found in the [`AmesHousing`](https://github.com/topepo/AmesHousing) package.

To pre-process the training set, we will use the _recipes_ package. We first eliminate predictors with sparse and highly unbalanced distributions (aka "near-zero variance predictors"), then estimate a transformation that will make the predictor distributions more symmetric. After these, the data are centered and scaled and the PCA components are computed. These same transformations will be applied to the new data points using the statistics estimated from the training set (e.g. PCA loadings, etc).

### Hat values scoring methods
```{r}
predictors <- mtcars[, -1]

# Data frame interface
mod <- apd_hat_values(predictors)

# Formula interface
mod2 <- apd_hat_values(mpg ~ ., mtcars)

# Recipes interface
rec <- recipe(mpg ~ ., mtcars)
rec <- step_log(rec, disp)
mod3 <- apd_hat_values(rec, mtcars)
```


