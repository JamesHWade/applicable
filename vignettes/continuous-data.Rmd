---
title: "Applicability domain methods for continuous data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{continuous-data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

```{r}
library(applicable)
```

`applicable` provides the following methods to analyze the applicability domain of your model:

* Principal component analysis
* Hat values statistics

## Example

We will use the Ames IA housing data for our example.

```{r ames_data, message=FALSE}
library(AmesHousing)
ames <- make_ames()
```

There are `r format(nrow(ames), big.mark = ",")` properties in the data.

The Sale Price was recorded along with `r ncol(ames)` predictors, including:

* Location (e.g. neighborhood) and lot information.
* House components (garage, fireplace, pool, porch, etc.).
* General assessments such as overall quality and condition.
* Number of bedrooms, baths, and so on.

More details can be found in [De Cock (2011, Journal of Statistics Education)](http://ww2.amstat.org/publications/jse/v19n3/decock.pdf).

The raw data are at [`http://bit.ly/2whgsQM`](http://bit.ly/2whgsQM) but we will use a processed version found in the [`AmesHousing`](https://github.com/topepo/AmesHousing) package.

To pre-process the training set, we will use the _recipes_ package. We first eliminate predictors with sparse and highly unbalanced distributions (aka "near-zero variance predictors"), then estimate a transformation that will make the predictor distributions more symmetric. After these, the data are centered and scaled and the PCA components are computed. These same transformations will be applied to the new data points using the statistics estimated from the training set.


```{r prep_data, message=FALSE}
library(recipes)
library(rsample)
library(dplyr)

training_data <- ames

training_recipe <-
  recipe( ~ ., data = training_data) %>%
  step_dummy(all_nominal()) %>%
  # Remove variables that have (or almost have) the same value for every data point.
  step_nzv(all_predictors()) %>%
  # Transform variables to be distributed as Gaussian-like as possible.
  step_YeoJohnson(all_predictors()) %>%
  # Normalize numeric data to have a mean of zero.
  step_center(all_predictors()) %>%
  # Normalize numeric data to have a standard deviation of one.
  step_scale(all_predictors()) %>%
  prep(strings_as_factors = FALSE)

```


### Principal Component Analysis

The following functions in `applicable` are used for principal component
analysis:

* `apd_pca`: computes the principal components that account for up
to either 95% or the provided `threshold` of variability. It alsomcomputes the
percentiles of the principal components and the mean of each principal
component.
* `autoplot`: plots the distribution function for pcas. You can also provide an
optional set of `dplyr` selectors, such as `dplyr::matches()` or
`dplyr::starts_with()`, for selecting which variables should be shown in the
plot.
* `score`: calculates the principal components of the new data and their
percentiles as compared to the training data. The number of principal
components computed depends on the `threshold` given at fit time. It also
computes the multivariate distance between each principal component and its
mean.

Let us apply `apd_pca` modeling function to our data:

```{r}
ames_pca <- apd_pca(training_recipe, training_data)
ames_pca
```

Since no `threshold` was provided, the function computed the number of
principal components that accounted for at most 95% of the total variance.

Setting `threshold = 0.43` or 43%, we now need only 10 principal components:

```{r}
ames_pca <- apd_pca(training_recipe, training_data, threshold = 0.43)
ames_pca
```

Plotting the distribution function for pcas is also helpful:

```{r, fig.width=5, fig.height=5.2,  out.width = '50%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}
library(ggplot2)
autoplot(ames_pca)
```

You can use regular expressions to plot a smaller subset of the pca statistics:

```{r, echo = FALSE}
par(mfrow=c(1,2))
autoplot(ames_pca, matches("PC0[1-5]"))
autoplot(ames_pca, distance) + scale_x_log10()
```

The `score` function compares the training data to new samples. We will
engineer a new fake sample that is an extrapolation of the training data.

```{r fake_sample}

training_df <- juice(training_recipe)
training_df_ad <- apd_pca(x = training_df)

set.seed(1)
s1 <- sample(x = c(0.1, 0.2, 0.254, 0.123), size = ncol(training_df), replace = TRUE)
fake_sample <- matrix(s1, nrow = 1)
colnames(fake_sample) <- colnames(training_df)
fake_sample <- as.data.frame(fake_sample)

pca_score <- score(training_df_ad, fake_sample)
pca_score

```

Notice how the new sample, displayed in red, is very disimilar to the training
set in the first two components:

```{r, echo = FALSE}

training_pca <- data.frame("PC1" = training_df_ad$pcs$rotation[,1],
                           "PC2" = training_df_ad$pcs$rotation[,2])

testing_pca <- data.frame("PC1" = pca_score$PC01,
                          "PC2" = pca_score$PC02)

all_pca <- rbind(training_pca, testing_pca)
pca_rng <- extendrange(c(all_pca$PC1, all_pca$PC2), f=c(.03,.01))

training_pca_plot_with_new_sample_reality <-
  ggplot(training_pca, aes(x = PC1, y = PC2)) +
  geom_point(alpha = .1) +
  lims(x = pca_rng, y = pca_rng) +
  theme(legend.position = "none") +
  geom_point(data = testing_pca, col = "red", cex = 3)

training_pca_plot_with_new_sample_reality

```


### Hat Values

The following functions in `applicable` are used to compute the hat values of your model:

* `apd_hat_values`:
* `score`:

Let us apply `apd_hat_values` modeling function to our data:

```{r}

# Data frame interface
ames_hat <- apd_hat_values(training_df)

X <- as.matrix(training_df)
colnames(X) <- NULL
xtx <- t(X) %*% X
det(xtx)

```

